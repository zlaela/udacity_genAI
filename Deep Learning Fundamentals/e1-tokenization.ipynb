{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Terms\n",
    "`Large Langauge Models (LLMS)` - AI models specificlaly trained to understand and generate human language.\n",
    "`Variational Autoencoders (VAE)` - AI models that can be used to create new images. Made up of 2 parts: The **encoder** reduces data to a simpler form, and the **decoder** expands it back to generate new content.\n",
    "`Parameters` - The varaibles that th emdoel learns during training. They are internal to the model and are adjusted through the learning process. In Neural Networks, they typically include weights and biases.\n",
    "`Weights` - coefficients for the input data. Used in calclulations to dtermine the importance or influcence of input variables to the model's output. In a NN, each connection between neurons has an associated weight.\n",
    "`Biases` - Additional constants attached to neurons and are added to the weighted input before the activation funciton is applied. Biases ensure a non-zero output when all in the inputs are zero.\n",
    "`Hyperparameters` - Unlike Parameters, theese are not learned from the data, but are more like settings or configurations for the learning process. THey're set prior to the training and remain constant throughout. External to the model and used to control training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Subword Tokenization is breaking down words into smalelr unites, such as parts of words or whole words.\n",
    "`tokenizer` - Tokenizes text\n",
    "`AutmodelForAausalLM.from_pretrained` - Loads a pretrained model\n",
    "`AutoTokenizer.from_pretrained` - Loads a tokenizer\n",
    "`torch.nn.functional.softmax` - Converts logits into probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer and model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a pretrained model and tokenizer using Huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create a partial sentence and tokenize it\n",
    "text = \"Following examples is the best way to learn about generative\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# showw the tokenns as numbers (their ID)\n",
    "inputs[\"input_ids\"]\n",
    "\n",
    "# output: tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Examining the tokens - what do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "\n",
    "def show_tokenizations(inputs):\n",
    "    return pd.DataFrame(\n",
    "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
    "        columns = [\"id\", \"token\"]\n",
    "    )\n",
    "\n",
    "show_tokenizations(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "2. Calculate the probablitly of the next token\n",
    "- Tokens are not just letters nor words, they can represent words, parts of words, or punctionations. This is called *subword tokenization*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcuate the probabilities for the next token for all possible choices. Show the top 5 and the corresponding words or subwords for those 5 tokens.\n",
    "import torch\n",
    "\n",
    "with torch.no_grad(): # no_grad() means this will not be used for training\n",
    "    logits = model(**inputs).logits[:,-1,:]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "def show_next_token_choices(probabilities, top_n=5):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (id, tokenizer.decode(id), p.item())\n",
    "            for id, p in enumerate(probabilities)\n",
    "            if p.item()\n",
    "        ],\n",
    "        columns =[\"id\",\"token\",\"p\"],\n",
    "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
    "\n",
    "show_next_token_choices(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the token id for the most probable next token\n",
    "next_token_id= torch.argmax(probabilities).item()\n",
    "print(f\"next token id: {next_token_id}\")\n",
    "print(f\"Next token: {tokenizer.decode(next_token_id)}\")\n",
    "\n",
    "# Append the most likely next token to the original text\n",
    "text = text + tokenizer.decode(8300) #8300 was the id of most likely token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Show the original text\n",
    "print(text)\n",
    "\n",
    "# Convert to tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Calculate the probabilities of the next token and show the top 5 choices\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=0)\n",
    "\n",
    "display(Markdown(\"**Next token probabilities:**\"))\n",
    "display(show_next_token_choices(probabilities))\n",
    "\n",
    "# Choose the most likely token ID and add it to the text\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "text = text + tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Generate method\n",
    "from IPython.display import Markdown,display\n",
    "\n",
    "#start with some text and tokenize it\n",
    "text = \"Once upon a time, generative models\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Use the 'generate' method to generate a bunch of text\n",
    "## This causes kernel crash locally, so not run\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Show the generated text\n",
    "display(Markdown(tokenizer.decode(output[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
