{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pytorch tensors\n",
    "Create a tensor of size 3x3 with any values. It should be created on the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device to be used for the tensor\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.device(device)\n",
    "\n",
    "# Create a tensor on the appropriate device\n",
    "my_tensor = torch.tensor([[5,6,7], [1,2,3], [0,9,8]])\n",
    "\n",
    "# print the tensor\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the previous cell\n",
    "assert my_tensor.device.type in {\"cuda\", \"cpu\"}\n",
    "assert my_tensor.shape == (3, 3)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Neural Net Constructor Kit `torch.nn`\n",
    "\n",
    "The `torch.nn` module can be thought of as a constructor kit for neural networks. It provides the building blocks for creating NNs, including layers, activation functions, and more.\n",
    "\n",
    "Create a 3-layer Multi-Layer Perceptron (MLP) neural network with the specs:\n",
    " - input layer: 784 neurons\n",
    " - hidden layer: 128 neurons\n",
    " - output layer: 10 neurons\n",
    "\n",
    "Use the ReLU activation function for the hidden layer and the softmax activation functio for the output layer. Print the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128) # 784 input, 128 output (input layer, hidden layer)\n",
    "        self.fc2 = nn.Linear(128, 10) # 128 input, 10 output (output layer)\n",
    "        self.relu = nn.ReLU()\n",
    "        self. softmax = nn.Softmax(dim=1) # dim=1 means the softmax will be applied to the 1st dimension\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input to the second layer\n",
    "        x = self.fc1(x)\n",
    "        # Apply the ReLU activation function\n",
    "        x = self.relu(x)\n",
    "        # Pass the result to the final layer\n",
    "        x = self.fc2(x)\n",
    "        # Apply the softmax activation function\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "my_mlp = MyMLP()\n",
    "print(my_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of inputs\n",
    "assert my_mlp.fc1.in_features == 784\n",
    "\n",
    "# Check the number of outputs\n",
    "assert my_mlp.fc2.out_features == 10\n",
    "\n",
    "# Check the number of nodes in the hidden layer\n",
    "assert my_mlp.fc1.out_features == 128\n",
    "\n",
    "# Check that my_mlp.fc1 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc1, nn.Linear)\n",
    "\n",
    "# Check that my_mlp.fc2 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc2, nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Loss Functions and Optimizers\n",
    "Pytorch comes with a number of built-in loss functions and optimizers that can be used to train NNs. Loss functions are implemented in the `torch.nn` module, and optimizers in the `torch.optim` module.\n",
    "\n",
    "Task:\n",
    " - Create a loss function using the `torch.nn.CrossEntropyLoss` class.\n",
    " - Create an optimizer using the `torch.optim.SGD` class with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (by convention, us the var 'optimizer')\n",
    "optimizer = optim.SGD(my_mlp.parameters(), lr=0.01)\n",
    "\n",
    "# Check\n",
    "assert isinstance(loss_fn, nn.CrossEntropyLoss), \"loss_fn should be a instance of CrossEntropyLoss\"\n",
    "assert isinstance(optimizer, torch.optim.SGD), \"optimizer should be a instance of SGD\"\n",
    "assert optimizer.defaults[\"lr\"] == 0.01, \"learning rate should be 0.01\"\n",
    "assert optimizer.param_groups[0][\"params\"] == list(my_mlp.parameters()), \"Optimizer should used passed MLP parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Training loops\n",
    "Training loops are easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_training_loaders():\n",
    "    for _ in range(30):\n",
    "        yield torch.randn(64, 784), torch.randint(0, 10, (64,)) # 64 is the batch size, 784 is the number of features, 10 is the number of classes\n",
    "\n",
    "for epoch in range(3):\n",
    "    # training loop\n",
    "    for i, data in enumerate(fake_training_loaders()):\n",
    "        # Every data instance is an input + lable pair\n",
    "        x, y = data\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (predictions)\n",
    "        y_pred = my_mlp(x)\n",
    "\n",
    "        # Calculate the loss and its gradients\n",
    "        loss = loss_fn(y_pred, y) # Compare prediction with the actual label\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item():.5f}\")\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "# Check\n",
    "assert abs(loss.item() - 2.3) < 0.1, \"the loss should be around 2.3 with random data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Download a model from HuggingFace and use it for sentiment analysis\n",
    "Hugging face provides pre-tratine models that can be used for a variety of tasks. This exercise uses the `distilbert-base-uncased-finetuned-sst-2-english` model to perform sentiment analysis on a movie review.\n",
    "\n",
    "Task:\n",
    " - Instantiate an AutoModelForSequenceClassification model using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
    " - Instantiate an AutoTokenizer using the model.\n",
    " - Define a function that will get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "#device_map=\"auto\" automatically allocates the model weights to your fastest device first, which is typically the GPU.\n",
    "#torch_dtype=\"auto\" directly initializes the model weights in the data type theyâ€™re stored in, which can help avoid loading the weights twice (PyTorch loads weights in torch.float32 by default).\n",
    "\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def get_prediction(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = pt_model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return \"positive\" if predictions.item() == 1 else \"negative\"\n",
    "    # return pt_model.config.id2label[predicted_class_id]\n",
    "\n",
    "# print(get_prediction(\"This movie was great!\")) << this crashes the kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Download a dataset from HuggingFace\n",
    "HuggingFace provides a number of datasets that can be used for various taks. Use the `imdb` dataset and pass it to the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # may require pip install datasets\n",
    "\n",
    "# Load the test split of the dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "from pprint import pprint\n",
    "from datasets import Dataset\n",
    "\n",
    "assert isinstance(dataset, Dataset), \"The dataset should be a Dataset object\"\n",
    "assert set(dataset.features.keys()) == {\n",
    "    \"label\",\n",
    "    \"text\"\n",
    "}, \"The dataset should have a label and a text feature\"\n",
    "\n",
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Use the pre-trained model!\n",
    "Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last 3 reviews\n",
    "reviews = dataset[\"text\"][-3:]\n",
    "\n",
    "# Get the last 3 labels\n",
    "labels = dataset[\"label\"][-3:]\n",
    "\n",
    "# Check\n",
    "for review, label in zip(reviews, labels):\n",
    "    prediction = get_prediction(review)\n",
    "\n",
    "    print(f\"Review: {review[:80]}\\n... {review[-80:]}\")\n",
    "    print(f'Label: {\"Positive\" if label else \"negative\"}')\n",
    "    print(f\"Prediction: {prediction}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
