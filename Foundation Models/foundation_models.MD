# Foundation Models

A foundation model is an AI tool that can do many different things after being trained on a lot of diverse data. Thees models are versatile and provide a base for creating various AI applications and specialized AI tasks.

- `Foundation Model` - A large AI model trained on a wide variety of data, which can do many tasks without much additional training.
- `Adapted` - Modified or adjusted to suit new conditions or a new purpose, i.e. in the context of foundation models.
- `Generalize` - The ability of a model to apply what it has learned from its training data to new, unseen data.

## Transformer Architecture

The transformer architecture has revolutionized the way machines handle language by enableing the training of sequential data at scale. Thanks to this, today's AI models are massive, some with billions of parameters (or more), allowing for flexibility across many tasks.

- `Sequential data`: Information that is arranged in a specific order, such as words in a sentence or events in time.
- `Self-attention mechanism`: The self-attention mechanism in a transformer is a process where each element in a sequence computes its representation by attending to and weighing the importance of all elements in the sequence, allowing the model to capture complex relationships and dependencies.